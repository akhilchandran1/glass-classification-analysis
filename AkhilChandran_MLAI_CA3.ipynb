{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I am using “glass_data.csv” dataset in order to perform the following analysis with the use of neural network:\n",
    "\n",
    "For the purpose of performing an initial analysis of the data(EDA), In order to implement the necessary data preparation, For creating and executing a neural network that will produce a classification depending on the Type of glass(class attribute) feature, For testing and improving the model using various configurations of neurons/layers/loss functions/activation functions, For creating a classification using test data with  the use of final neural network configuration.\n",
    "\n",
    "# Exploratory Data Analysis(EDA)\n",
    "\n",
    "The process used to examine or understand the data and extract insights or main characteristics of data is known as EDA. It is classified into two methods which are graphical analysis and non -graphical analysis. Histograms, Box plots, Scatter plot and others are all used for plotting in EDA. Exploration of data mostly takes a lot of time. It is possible to define the problem statement or most importantly define our data set using the process of EDA.\n",
    "\n",
    "Exploratory Data Analysis being the first step contributes to gaining an insight into a data set, understanding the underlying structure, extracting important parameters and the relationships present between them and also test underlying assumptions.\n",
    "\n",
    "##### The main purpose of EDA is \n",
    "Assessing the data distribution\n",
    "\n",
    "Managing missing values of the dataset (it is a very common issue with most datasets)\n",
    "\n",
    "Eliminating duplicate data\n",
    "\n",
    "Managing outliers\n",
    "\n",
    "Encoding the categorical variables\n",
    "\n",
    "Normalizing and scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "###  DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries \n",
    "\n",
    "Initially all the necessary python libraries will be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "from IPython.core.display import display\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Input\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential # Neural network library\n",
    "from keras.layers import Dense # layer library\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "#### Reading Data\n",
    "\n",
    "A very important step in EDA is loading the data into the pandas data frame. The pandas library function of read_csv() is used to read the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Database/glass_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five observations from the data set are returned using “.head()” function of the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset and assign column names\n",
    "As we observe that the columns don’t have names we load the dataset and assign the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Database/glass_data.csv', names=['Id_number', 'RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type_of_glass' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five observations from the data set are returned using “.head()” function of the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last five observations from the data set are returned using “.tail()” function of the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "#### Observe the dimensions\n",
    "\n",
    "The dimensions of the data i.e. total number of rows and columns can be observed using shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "#### Checking the data types\n",
    "\n",
    "The info() function is used to check the types of data and find columns it contains, its types, whether it contains any value in it or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(dataset.info())\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the above data, it can be concluded that the data consists of 9 floats,2 integer values and that all column variables are non-null which means no empty or missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "The describe() method is used provides the count, mean, standard deviation, minimum and maximum values and the quantities of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "#### Handling missing values\n",
    "\n",
    "The missing values in the dataset needs to be handled.  As it turns out there are no missing values in this dataset but that is not the case in real world.\n",
    "\n",
    "We need to check the presence of any null values and print them if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(dataset.isnull().sum())\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print('Number of Null Values: ',dataset.isnull().sum().sum())\n",
    "print('\\n',\"_\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe that there are no null values in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check if there are any wrong type or missing type we need to group by type of glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby(['Type_of_glass']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using unique() function in order to find the unique value of the Type of glass column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Type_of_glass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be confirmed that ‘type4’ is missing from the Type of glass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the data counts in type of glass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Type_of_glass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "\n",
    "#### Checking for duplicate values\n",
    "\n",
    "Checking for presence of duplicate values in our dataset as it will affect the accuracy of our ML model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = dataset.duplicated()\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(\"Number of Duplicate Values: \",duplicate.sum())\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "dataset[duplicate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that there are no duplicate values in our dataset.\n",
    "\n",
    "Now we select duplicate rows depending on list of column names as there could be duplicate entries having different ID(normally consecutive). Then the above method will not recognise it as duplicate due to it different ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = dataset[dataset.duplicated(['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type_of_glass'])]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the rows having same values in order to verify if the ID is consecutive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.duplicated(['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type_of_glass'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consecutive Now we know that the ID numbers of duplicate values are consecutive\n",
    "\n",
    "#### Handling duplicate values\n",
    "\n",
    "By using drop_duplicates() the duplicate values can be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the dataset shape before remove the duplicate values\n",
    "print('Dataset shape before remove duplicate values: ', dataset.shape)\n",
    "\n",
    "# remove the duplicate values \n",
    "dataset.drop_duplicates(['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type_of_glass'],inplace=True)\n",
    "\n",
    "# printing the dataset shape after remove the duplicate values\n",
    "print('Dataset shape after remove duplicate values: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = dataset[dataset.duplicated(['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type_of_glass'])]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have handled the duplicate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "\n",
    "#### Removing unnecessary columns\n",
    "\n",
    "Drop the ID Number column as it is of no use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the ID Number as it is of no use.\n",
    "dataset.drop('Id_number', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see we successfully droped the Id_number column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10\n",
    "\n",
    "#### Handling the outliers\n",
    "\n",
    "Managing the outliers i.e. the extreme values in the data. With the use of boxplot and histplot the outliers in our data can be identified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making a copy of dataset and replacing the ‘Type of glass’ column values (numbers) using real names \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax:DataFrame.copy ( deep=True)\n",
    "\n",
    "In the case of deep=True(default), a new object will be created ith a copy of the calling object’s data and indices. The original objectwill not reflect the modifications made to the data or indices of the copy(see notes below).\n",
    "\n",
    "A new object will be created without copying the calling object’s data or index when deep=False (only references to data and index are copied). The changes made in the data of original will be reflected in the shallow copy and vice-versa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.copy(deep=True)\n",
    "df['Type_of_glass'] = df['Type_of_glass'].replace([1,2,3,4,5,6,7],['Building windows float processed', 'Building windows non float processed', 'Vehicle windows float processed', 'Vehicle windows non float processed (none in this database)', 'Containers', 'Tableware', 'Headlamps'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets group by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Type_of_glass']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the percentage of each Type_of_glass category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(df.Type_of_glass.value_counts(normalize=True))\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the bar graph of percentage Type_of_glass categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "df.Type_of_glass.value_counts(normalize=True).plot.barh(figsize= (18,5), fontsize=20, color=['Red','Pink','LightBlue','Purple','Brown','Green'])\n",
    "plt.show()\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating plot_hist_box function to make hist and box plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_box(name,x_lbl):\n",
    "    fig, axes = plt.subplots(1,2,figsize=(27,5))\n",
    "    sns.set(font_scale = 2)\n",
    "    plt.tight_layout()\n",
    "    sns.histplot(df[name], ax = axes[0])\n",
    "    axes[0].set_xlabel(x_lbl, fontsize=20)\n",
    "    axes[0].set_ylabel('Count', fontsize=20)\n",
    "    axes[0].yaxis.tick_left()\n",
    "\n",
    "    sns.boxplot(x = dataset['Type_of_glass'], y = name, data = df, hue = 'Type_of_glass', palette=\"Set1\", dodge = False, ax = axes[1])\n",
    "    axes[1].set_xlabel('Type of glass', fontsize=20)\n",
    "    axes[1].set_ylabel(x_lbl, fontsize=20)\n",
    "    axes[1].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    axes[1].yaxis.set_label_position(\"right\")\n",
    "    axes[1].yaxis.tick_right()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refractive Index\n",
    "plot_hist_box('RI','Refractive Index')\n",
    "\n",
    "# Sodium\n",
    "plot_hist_box('Na', 'Sodium')\n",
    "\n",
    "# Magnesium\n",
    "plot_hist_box('Mg', 'Magnesium')\n",
    "\n",
    "# Aluminum\n",
    "plot_hist_box('Al', 'Aluminum')\n",
    "\n",
    "# Silicon\n",
    "plot_hist_box('Si', 'Silicon')\n",
    "\n",
    "# Potassium\n",
    "plot_hist_box('K', 'Potassium')\n",
    "\n",
    "# Calcium\n",
    "plot_hist_box('Ca', 'Calcium')\n",
    "\n",
    "# Barium\n",
    "plot_hist_box('Ba', 'Barium')\n",
    "\n",
    "# Iron\n",
    "plot_hist_box('Fe', 'Iron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a function so that it can be called to check after handling the outliers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(otlr_df):\n",
    "    plotlocation = 0\n",
    "    plt.figure(figsize=(25,15))\n",
    "    sns.set(font_scale = 2)\n",
    "    columns = otlr_df.columns.values\n",
    "    for column in columns:\n",
    "        plotlocation = plotlocation + 1\n",
    "        plt.subplot(2,5,plotlocation)\n",
    "        plt.tight_layout()\n",
    "        otlr_df.boxplot(column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(dataset.loc[:,dataset.columns != 'Type_of_glass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above boxplot it is observed that the normal range of data lies within the block and the outliers are indicated by the small circles in the extreme end of the graph\n",
    "\n",
    "In order to handle it we can either drop the outliers value or replace it using IQR (Interquartile Range Method).\n",
    "\n",
    "dropping the outliers cannot be afforded when we have less data points hence we replace the outlier values using IQR.\n",
    "\n",
    "The difference between the 25th and 75th percentile of the data gives the IQR. By sorting the selecting values at specific indices the percentiles can be calculated. The IQR is used to identify outliers by defining limits on the sample values that are a factor k of the IQR. 1.5 is the common value for the factor k .\n",
    "\n",
    "\n",
    "Creating a copy of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_outlier_removed = dataset.copy(deep=True)\n",
    "dataset_outlier_removed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets find the IQR (Inter quantile range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundaries(df,variables):\n",
    "    # lets find the IQR (Inter quantile range)\n",
    "    Q1 = df[variables].quantile(0.25)\n",
    "    Q3 = df[variables].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_boundary = Q1 - (1.5 * IQR)\n",
    "    upper_boundary = Q3 + (1.5 * IQR)\n",
    "    \n",
    "    return lower_boundary, upper_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the lower and upper limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_RI, upper_RI = find_boundaries(dataset_outlier_removed, 'RI')\n",
    "lower_Na, upper_Na = find_boundaries(dataset_outlier_removed, 'Na')\n",
    "lower_Mg, upper_Mg = find_boundaries(dataset_outlier_removed, 'Mg')\n",
    "lower_Al, upper_Al = find_boundaries(dataset_outlier_removed, 'Al')\n",
    "lower_Si, upper_Si = find_boundaries(dataset_outlier_removed, 'Si')\n",
    "lower_K, upper_K = find_boundaries(dataset_outlier_removed, 'K')\n",
    "lower_Ca, upper_Ca = find_boundaries(dataset_outlier_removed, 'Ca')\n",
    "#lower_Ba, upper_Ba = find_boundaries(dataset_outlier_removed, 'Ba')\n",
    "lower_Fe, upper_Fe = find_boundaries(dataset_outlier_removed, 'Fe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lower limit of RI is: ', lower_RI,'\\nUpper limit of RI is: ', upper_RI)\n",
    "print('Lower limit of Na is: ', lower_Na,'\\nUpper limit of Na is: ', upper_Na)\n",
    "print('Lower limit of Mg is: ', lower_Mg,'\\nUpper limit of Mg is: ', upper_Mg)\n",
    "print('Lower limit of Al is: ', lower_Al,'\\nUpper limit of Al is: ', upper_Al)\n",
    "print('Lower limit of Si is: ', lower_Si,'\\nUpper limit of Si is: ', upper_Si)\n",
    "print('Lower limit of K is: ', lower_K,'\\nUpper limit of K is: ', upper_K)\n",
    "print('Lower limit of Ca is: ', lower_Ca,'\\nUpper limit of Ca is: ', upper_Ca)\n",
    "#print('Lower limit of Ba is: ', lower_Ba,'\\nUpper limit of Ba is: ', upper_Ba)\n",
    "print('Lower limit of Fe is: ', lower_Fe,'\\nUpper limit of Fe is: ', upper_Fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at upper and lower limits capping variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capping_ver(ver,upper,lower):\n",
    "    dataset_outlier_removed[ver] = np.where(dataset_outlier_removed[ver] > upper, upper,\n",
    "                          np.where(dataset_outlier_removed[ver] < lower, lower, dataset_outlier_removed[ver]))\n",
    "    return dataset_outlier_removed[ver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capping_ver('RI',upper_RI,lower_RI)\n",
    "capping_ver('Na',upper_Na,lower_Na)\n",
    "capping_ver('Mg',upper_Mg,lower_Mg)\n",
    "capping_ver('Al',upper_Al,lower_Al)\n",
    "capping_ver('Si',upper_Si,lower_Si)\n",
    "capping_ver('K',upper_K,lower_K)\n",
    "capping_ver('Ca',upper_Ca,lower_Ca)\n",
    "#capping_ver('Ba',upper_Ba,lower_Ba)\n",
    "capping_ver('Fe',upper_Fe,lower_Fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again plot the boxplot and check if we handled the outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(dataset_outlier_removed.loc[:,dataset.columns != 'Type_of_glass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see we successfully processed all the outliers except the 'Ba' column. which we didnt process because the column contains full of outliers and we loose data if we try to process it. \n",
    "\n",
    "to confirm we didn’t loose any data lets observe our dataset shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_outlier_removed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have the same shape which means we didn’t loose any data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing and Scaling – Data Normalization or feature scaling is A process used to standardize the range of features of the data as the range may vary a lot is known as data normalization or feature scaling. Thus we can preprocess the data using ML algorithms.\n",
    "\n",
    "### Normalizing and Scaling (StandardScaler, MinMaxScaler and RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import RobustScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# taking all the columns except Type_of_glass column\n",
    "data = dataset.loc[:,dataset.columns != 'Type_of_glass']\n",
    "# perform a robust scaler transform of the dataset\n",
    "scaler = preprocessing.RobustScaler()\n",
    "robust_df = scaler.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "robust_df = pd.DataFrame(robust_df, columns =['RI', 'Na', 'Mg', 'Al','Si','K','Ca','Ba','Fe'])\n",
    "\n",
    "# perform a Standard Scaler transform of the dataset\n",
    "scaler = preprocessing.StandardScaler()\n",
    "standard_df = scaler.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "standard_df = pd.DataFrame(standard_df, columns =['RI', 'Na', 'Mg', 'Al','Si','K','Ca','Ba','Fe'])\n",
    "\n",
    "# perform a MinMaxScaler transform of the dataset\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "minmax_df = scaler.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "minmax_df = pd.DataFrame(minmax_df, columns =['RI', 'Na', 'Mg', 'Al','Si','K','Ca','Ba','Fe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to plot the data that we scaled using StandardScaler, MinMaxScaler and RobustScaler to compare the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scale(df_type_1,df_type_2,title_1,title_2):\n",
    "    fig, axes = plt.subplots(1,2,figsize=(27,8))\n",
    "    # increasing font size\n",
    "    sns.set(font_scale = 2)\n",
    "    sns.kdeplot(df_type_1['RI'], ax = axes[0], label='RI')\n",
    "    sns.kdeplot(df_type_1['Na'], ax = axes[0], label='Na')\n",
    "    sns.kdeplot(df_type_1['Mg'], ax = axes[0], label='Mg')\n",
    "    sns.kdeplot(df_type_1['Al'], ax = axes[0], label='Al')\n",
    "    sns.kdeplot(df_type_1['Si'], ax = axes[0], label='Si')\n",
    "    sns.kdeplot(df_type_1['K'], ax = axes[0], label='K')\n",
    "    sns.kdeplot(df_type_1['Ca'], ax = axes[0], label='Ca')\n",
    "    sns.kdeplot(df_type_1['Ba'], ax = axes[0], label='Ba')\n",
    "    sns.kdeplot(df_type_1['Fe'], ax = axes[0], label='Fe')\n",
    "    axes[0].set_xlabel('data', fontsize=20)\n",
    "    axes[0].set_title(title_1, fontsize=25)\n",
    "    axes[0].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=5, mode=\"expand\", borderaxespad=0.)\n",
    "    axes[0].yaxis.set_label_position(\"left\")\n",
    "    axes[0].yaxis.tick_left()\n",
    "    \n",
    "    # increasing font size\n",
    "    sns.set(font_scale = 2)\n",
    "    sns.kdeplot(df_type_2['RI'], ax = axes[1], label='RI')\n",
    "    sns.kdeplot(df_type_2['Na'], ax = axes[1], label='Na')\n",
    "    sns.kdeplot(df_type_2['Mg'], ax = axes[1], label='Mg')\n",
    "    sns.kdeplot(df_type_2['Al'], ax = axes[1], label='Al')\n",
    "    sns.kdeplot(df_type_2['Si'], ax = axes[1], label='Si')\n",
    "    sns.kdeplot(df_type_2['K'], ax = axes[1], label='K')\n",
    "    sns.kdeplot(df_type_2['Ca'], ax = axes[1], label='Ca')\n",
    "    sns.kdeplot(df_type_2['Ba'], ax = axes[1], label='Ba')\n",
    "    sns.kdeplot(df_type_2['Fe'], ax = axes[1], label='Fe')\n",
    "    axes[1].set_xlabel('data', fontsize=20)\n",
    "    axes[1].set_title(title_2, fontsize=25)\n",
    "    axes[1].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=5, mode=\"expand\", borderaxespad=0.)\n",
    "    axes[1].yaxis.set_label_position(\"right\")\n",
    "    axes[1].yaxis.tick_right()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scale(data,robust_df,'Before Scaling\\n\\n\\n','After Robust Scaling\\n\\n\\n')\n",
    "plot_scale(standard_df,minmax_df,'After Standard Scaling\\n\\n\\n','After Min-Max Scaling\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Standard Scaling turns out to be better than others we are using it for further process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the shape of the data to makesure that we didnt loose any values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data shape before process: ',data.shape)\n",
    "print('Data shape After Robust Scaling: ',robust_df.shape)\n",
    "print('Data shape After After Standard Scaling: ',standard_df.shape)\n",
    "print('Data shape After After Min-Max Scaling: ',minmax_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he data has been normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add the Type_of_glass column back to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df['Type_of_glass'] = dataset_outlier_removed['Type_of_glass'].values\n",
    "#dataset_outlier_removed[dataset_outlier_removed.isnull().any(axis=1)]\n",
    "robust_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12\n",
    "\n",
    "#### Data Visualization and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \t\n",
    "\n",
    "#### hue\n",
    "\n",
    "Variable in data to map plot aspects to different colors.\t\n",
    "\n",
    "#### palette\n",
    "\n",
    "set of colours used for mapping hue variables\n",
    "\n",
    "\n",
    "#### kind\n",
    "\n",
    "plot used for non-identity relationships. {‘scatter’, ‘reg’}\n",
    "\n",
    "\n",
    "#### diag_kind\n",
    " plot used for the diagonal subplots. {‘hist’, ‘kde’}\n",
    "\n",
    "\n",
    "(tutorialspoint, 2021)\n",
    "\n",
    "lets make a Pairplot to visualizes the data to find the relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "sns.set(font_scale=1.8)\n",
    "g = sns.pairplot(standard_df, hue=\"Type_of_glass\",diag_kind = \"kde\",diag_kws={\"hue\": None, \"color\": \".2\"},kind = \"scatter\",palette = \"rainbow\")\n",
    "handles = g._legend_data.values()\n",
    "labels = g._legend_data.keys()\n",
    "g.fig.legend(title='Type_of_glass',handles=handles, labels=labels, loc='upper center', ncol=6)\n",
    "g.fig.legend(title='Type_of_glass',handles=handles, labels=labels, loc='lower center', ncol=6)\n",
    "g.fig.subplots_adjust(top=0.94, bottom=0.09)\n",
    "g._legend.set_bbox_to_anchor((0.4, 0.))\n",
    "g._legend.remove()\n",
    "plt.show()\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corr() method can be used to find the pairwise correlation between the different columns of the data.\n",
    "\n",
    "We find the pairwise correlation of all columns in the data frame using standard_df.corr() . e automatically exclude any ‘nan’ values.\n",
    "\n",
    "A value between -1 and 1 inclusive is the resulting coefficient, where:\n",
    "\n",
    "1: Total positive linear correlation\n",
    "0: No linear correlation, the two variables most likely do not affect each other\n",
    "-1: Total negative linear correlation\n",
    "\n",
    "\n",
    "Creating a heatmap using Seaborn for visualizing the correlation between the different columns of our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = standard_df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "#Plot figsize\n",
    "plt.figure(figsize=(17, 17))\n",
    "# increasing font size\n",
    "sns.set(font_scale = 1.5)\n",
    "#Generate Heat Map\n",
    "ax = sns.heatmap(corr, cmap=plt.cm.plasma, annot=True,linecolor=\"white\", square=True, fmt=\".2f\",linewidths=(1,1))\n",
    "\n",
    "plt.title(\"Correlations between the dimensions\\n\", fontsize=30)\n",
    "plt.xlabel(\"Different kind of glass\", fontsize=20)\n",
    "plt.ylabel(\"Different kind of glass\", fontsize=20)\n",
    "#Apply xticks\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(np.arange(len(corr.columns))+0.5, corr.columns)\n",
    "\n",
    "#Apply yticks\n",
    "plt.yticks(np.arange(len(corr.columns))+0.5, corr.columns)\n",
    "plt.yticks(rotation=0) \n",
    "#show plot\n",
    "plt.show()\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the relation using Seaborn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartlocation=0\n",
    "columns =['RI', 'Na', 'Mg', 'Al','Si','K','Ca','Ba','Fe']\n",
    "plt.figure(figsize=(17,14))\n",
    "for i in columns:\n",
    "    chartlocation = chartlocation + 1\n",
    "    plt.subplot(3,3,chartlocation)\n",
    "    plt.tight_layout()\n",
    "    sns.regplot(x=i,y='Type_of_glass',data = standard_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = standard_df.loc[:,dataset_outlier_removed.columns != 'Type_of_glass']\n",
    "target = standard_df.loc[:,'Type_of_glass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "\n",
    "The Refractive Index is correlated to the Ca Oxide Content quite heavily...\n",
    "the increase in CaO content or R ratio, heat‐treated glasses exhibit direct band gap within 5.92‐6.01 eV range. \n",
    "The Urbach energy lies within the 0.62‐0.86 eV range for all the heat‐treated glass samples.\n",
    "\n",
    "It is proved that the optical parameters of the glass are influenced by the CaO content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot figsize\n",
    "plt.figure(figsize=(17, 8))\n",
    "corr.iloc[0,:].plot(kind='bar', color=['Red','Pink','LightBlue','Purple','Brown','Green','Blue','DarkGreen','Yellow'])\n",
    "#show plot\n",
    "plt.show()\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An increase in calcium and iron content positively affects the refractive index of glass while rest of the elements have a negative correlation with the refractive index \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets have a look how other elements are correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartlocation = 0\n",
    "plt.figure(figsize=(17,18))\n",
    "columns = np.copy(corr.columns.values)\n",
    "for index, row in corr.iterrows():\n",
    "    column_name = columns[chartlocation]\n",
    "    chartlocation = chartlocation + 1\n",
    "    plt.subplot(4,3,chartlocation)\n",
    "    plt.tight_layout()\n",
    "    row.drop(index).plot(kind='bar', title='\\n'+column_name+'\\n', color=['Red','Pink','LightBlue','Purple','Brown','Green','Blue','DarkGreen','Yellow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only element that is negatively correlated with every other element that make up glass is silica. If there is an increase in silica every other element needs to be decreased. This increase mainly affects the refractive index as seen in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standard_df.drop(columns = ['Type_of_glass'], axis = 1)\n",
    "y = standard_df['Type_of_glass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.skew().plot(kind='bar', figsize=(17,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(15, 15))\n",
    "ax = Axes3D(fig)\n",
    "X_reduced = PCA(n_components=3).fit_transform(features)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y)\n",
    "plt.title(\"Priciple components 3\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1, figsize=(17, 8))\n",
    "X_reduced = PCA(n_components=2).fit_transform(features)\n",
    "plt.title(\"Priciple components 2\")\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training (80%) set and Test(20%) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets have a look at the shape of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of y_test:\",y_test.shape)\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Feature Scaling\n",
    "\n",
    "Now we have to scale our dataset using Sklearn’s StandardScaler. Due to the massive amounts of computations taking place in deep learning, feature scaling is compulsory. Feature scaling standardizes the range of our independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = preprocessing.StandardScaler()\n",
    "#X_train_scaled = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns.values)\n",
    "#X_test_scaled = pd.DataFrame(sc.fit_transform(X_test), columns=X_test.columns.values)\n",
    "\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.fit_transform(X_test)\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print('\\n',\"_\"*110,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one of the ensemble methods to find how important different elements are to the making of a glass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "std_df = standard_df.loc[:,standard_df.columns != 'Type_of_glass']\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train_scaled, np.ravel(y_train,order='C'))\n",
    "feature_with_importance = pd.DataFrame()\n",
    "feature_with_importance['columns'] = std_df.columns\n",
    "feature_with_importance['importance'] = clf.feature_importances_\n",
    "feature_with_importance.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "feature_with_importance.set_index('columns', inplace=True)\n",
    "feature_with_importance.plot(kind='bar',figsize=(17, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to encode output column\n",
    "def encode(data):\n",
    "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
    "    encoded = to_categorical(data)\n",
    "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = encode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded = encode(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = np.delete(y_train_encoded, [0,4], axis = 1)\n",
    "y_test_encoded = np.delete(y_test_encoded, [0,4], axis = 1)\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(y_train_encoded[2])\n",
    "print(y_test_encoded[2])\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14\n",
    "\n",
    "## Building the Artificial Neural Network(ANN)\n",
    "\n",
    "First we create a ANN model and we will improve the model.\n",
    "\n",
    "#### Adding input layer (First Hidden Layer)\n",
    "Different layers are added to our ANN using add method. The number of nodes is the fierst parameter to be added to this layer. No rule of thumb is applied to the number of nodes being added. The function used to initialize the waits is the second parameter, kernel_initializer. The number of nodes in the input layer is the final parameter input_dim. The number of independent variables are represented by this\n",
    "#### Adding Hidden Layers\n",
    " The input_shape parameter doesn’t need to specified while adding the second hidden layer as it is already specified in the first hidden layer. It was specified in the first hidden layer to let the layer know how many input nodes to expect. It doesn’t need to be repeated as in the second hidden layer the ANN is ware of the number of input nodes to be expected.\n",
    "#### Adding the output layer\n",
    "In this case there are 6 classes. The first parameter is changed to 6(unit=6) and the activation function is changed to softmax. Softmax is a sigmoid function that is applied to an independent variable having more than two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising ANN by creating an instance of Sequential\n",
    "first_model = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "first_model.add(Dense(units = 18, input_shape=(9,), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "# Adding the second hidden layer\n",
    "first_model.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "first_model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the ANN\n",
    "Compiling means basically applying a stochastic gradient descent to the whole neural network. The first parameter is the algorithm we prefer to use in order to get the optimal set of weights in the neural network. This has many variants and an efficient one to use is Adam. The loss function is the second parameter and here we use the categorical_crossentopy loss function. The final argument comprises of the criterion used to evaluate our model. In this case we use the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN Model \n",
    "# Compiling the ANN model\n",
    "first_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "first_model_history = model_01.fit(X_train_scaled,\n",
    "                                   y_train_encoded,\n",
    "                                   validation_data=(X_test_scaled, y_test_encoded),\n",
    "                                   batch_size = 10,\n",
    "                                   epochs = 500)\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(\"Training set - first_model: \", first_model_history.history.get('accuracy')[-1])\n",
    "print(\"Test set - first_model: \", first_model_history.history.get('val_accuracy')[-1])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a function to make plot visualize the models train, test accuracys and train, test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_lss_oneTry(model,title):\n",
    "    plt.figure(figsize = (17,8))\n",
    "    plt.plot(model.history[\"accuracy\"], label = \"Training Accuracy\")\n",
    "    plt.plot(model.history[\"val_accuracy\"], label = \"Validation Accuracy\")\n",
    "    plt.plot(model.history[\"loss\"], label = \"Training Loss\")\n",
    "    plt.plot(model.history[\"val_loss\"], label = \"Validation Loss\")\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(title+\"\\n\\n\\n\\n\")\n",
    "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize to get a clear understanding of models accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_lss_oneTry(first_model_history,\"Model Accuracy and Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above visualisation we can understand that the validation loss is really high especially after 200 epochs\n",
    "\n",
    "lets print the final loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_\"*110)\n",
    "print('\\n First Model\\n')\n",
    "final_loss_fm1, final_accuracy_fm1 = first_model.evaluate(X_test_scaled, y_test_encoded)\n",
    "print('Final Loss: {}, Final Accuracy: {}'.format(final_loss_fm1, final_accuracy_fm1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets make a plot to visualize the pridicted and actual values (Type of glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glasses = ['Building windows\\n float processed', 'Building windows\\n non float processed', 'Vehicle windows\\n float processed', 'Containers', 'Tableware', 'Headlamps']\n",
    "Y_true = np.argmax(y_test_encoded, axis=1)\n",
    "# Model 01\n",
    "Y_pred = first_model.predict(X_test_scaled)\n",
    "\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "cm = confusion_matrix(Y_true, Y_pred)\n",
    "plt.figure(figsize=(20, 10))\n",
    "# increasing font size\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.heatmap(cm, cmap=\"RdYlGn\" , annot=True, square=True, xticklabels=glasses, yticklabels=glasses,linewidths=.5)\n",
    "ax.set_ylabel('Actual', fontsize=20)\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "plt.title('Model Accuracy\\n', fontsize=30)\n",
    "plt.show()\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see we dont have much accurate prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(\"Calculating first model accuracy\")\n",
    "scores = first_model.evaluate(X_test_scaled, y_test_encoded)\n",
    "print(f\"First Model - Test Accuracy: {scores[1]*100}\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is not good and the accuracy is really low. so we need to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15\n",
    "\n",
    "#### Defining a function to pass by build_fn argument\n",
    "\n",
    "KerasClassifier class in Keras takes an argument build_fn which is the name of the function to call to get your model.\n",
    "We must define a function that defines our model, compiles it and returns it.\n",
    "\n",
    "we need to initialize our ANN by creating an instance of Sequential. The Sequential function initializes a linear stack of layers. This allows us to add more layers later using the Dense module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding input layer (First Hidden Layer)\n",
    "Different layers are added to our ANN using add method. The number of nodes is the fierst parameter to be added to this layer. No rule of thumb is applied to the number of nodes being added. The function used to initialize the waits is the second parameter, kernel_initializer. The number of nodes in the input layer is the final parameter input_dim. The number of independent variables are represented by this\n",
    "#### Adding Hidden Layers\n",
    " The input_shape parameter doesn’t need to specified while adding the second hidden layer as it is already specified in the first hidden layer. It was specified in the first hidden layer to let the layer know how many input nodes to expect. It doesn’t need to be repeated as in the second hidden layer the ANN is ware of the number of input nodes to be expected.\n",
    "#### Adding the output layer\n",
    "In this case there are 6 classes. The first parameter is changed to 6(unit=6) and the activation function is changed to softmax. Softmax is a sigmoid function that is applied to an independent variable having more than two categories.\n",
    "#### Compiling the ANN\n",
    "Compiling means basically applying a stochastic gradient descent to the whole neural network. The first parameter is the algorithm we prefer to use in order to get the optimal set of weights in the neural network. This has many variants and an efficient one to use is Adam. The loss function is the second parameter and here we use the categorical_crossentopy loss function. The final argument comprises of the criterion used to evaluate our model. In this case we use the accuracy.\n",
    "\n",
    "\n",
    "Defining function named as build_model_01 :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_01():\n",
    "    # Initialising Model\n",
    "    model = Sequential()\n",
    "    # Adding the input layer and first hidden layer\n",
    "    model.add(Dense(units = 18, input_shape=(9,), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    # Adding the hidden layer\n",
    "    model.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    model.summary()\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will build the model and return it for use in the next step.\n",
    "\n",
    "\n",
    "\n",
    "#### Creating model with default batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_01 = KerasClassifier(build_fn = build_model_01, epochs=100)\n",
    "model_01.evaluate= \"classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting model\n",
    "\n",
    "X_train_scaled represents the independent variables we’re using to train our ANN, and y_train_encoded represents the column we’re predicting. Epochs represents the number of times we’re going to pass our full dataset through the ANN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_01 = model_01.fit(X_train_scaled,\n",
    "                          y_train_encoded, \n",
    "                          validation_data=(X_test_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above output:\n",
    "\n",
    "We saw that 100 epochs and 210 iterations for each epoch.Because the default batch size is 32; we had 164 samples / 32 = 5.12 (5 or 6) batches for each epoch. Parameters (weights and bias) were updated and accuracy re-calculated after each batch in each epoch.\n",
    "\n",
    "For example: in 1st epoch, parameters and accuracy calculated (with 32 samples) after 1st batch (1/6).\n",
    "\n",
    "Then parameters and accuracy re-calculated (with 32 samples) after 2nd batch (2/6).\n",
    "\n",
    "Then parameters and accuracy re-calculated (with 32 samples) after 3rd batch (3/6) and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating model with a decreased batch_size\n",
    "\n",
    "The only thing we have done here is added batch_size = 10. Batch_size is the number of observations after which the weights will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_02 = KerasClassifier(build_fn = build_model, \n",
    "                           epochs=100, \n",
    "                           batch_size = 10)\n",
    "model_02.evaluate= \"classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_02 = model_02.fit(X_train_scaled, \n",
    "                          y_train_encoded, \n",
    "                          validation_data=(X_test_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing batch size will increase the iteration number as well as computation time and cpu usage.\n",
    "\n",
    "Now lets create a function to make plot visualize the models train, test accuracys and train, test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_lss(model_1, model_2, m1_label_1, m1_label_2, m2_label_1, m2_label_2, title):\n",
    "    plt.figure(figsize = (17,8))\n",
    "    plt.plot(model_1.history[\"accuracy\"], label = m1_label_1)\n",
    "    plt.plot(model_2.history[\"accuracy\"], label = m2_label_1)\n",
    "    plt.plot(model_1.history[\"loss\"], label = m1_label_2)\n",
    "    plt.plot(model_2.history[\"loss\"], label = m2_label_2)\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(title+\"\\n\\n\\n\\n\")\n",
    "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_lss(history_01,\n",
    "             history_02,\n",
    "             \"Batch size = 32 (Model_01)  Accuracy\",\n",
    "             \"Batch size = 32 (Model_01) Loss\",\n",
    "             \"Batch size = 10 (Model_02)  Accuracy\",\n",
    "             \"Batch size = 10 (Model_02) Loss\",\n",
    "             \"Affects of batch size on accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see batch size 10 is more accurate with less loss than batch 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model with cross_val_score and StratifiedKFold\n",
    "\n",
    "A new classifier is created using K-fold cross validation and the parameter build_fn is passed as the function created. After this the number of epochs and batch size is passed. Scikit-learn's cross_val_score function is used to apply the k-fold cross validation function. The model built with build_model is the estimator. The number of folds is cv. The accuracies of the test folds used in the computation will be returned with the cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = build_model, \n",
    "                        epochs=100, \n",
    "                        batch_size = 10)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = 4, \n",
    "                        shuffle = True, \n",
    "                        random_state = 42)\n",
    "\n",
    "accuracies = cross_val_score(estimator = model, \n",
    "                             X = X_train_scaled, \n",
    "                             y = y_train, \n",
    "                             cv = kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (10,6))\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel(\"K-fold values of Cross Validation Score\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.title(\"Cross Validation Accuracies vs K-Folds\")\n",
    "plt.grid(axis = \"both\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy : {} @ k-fold value of {}\".format(round(accuracies.max()*100,2),accuracies.argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fighting Overfitting\n",
    "\n",
    "When a model learns the details and noise in the training set such that it performs poorly on the test set it is known as overfitting in machine learning. Where there are huge differences between the accuracies of test and training set or high variance is observed when the k-fold cross validation is applied it can be observed. Dropout regularization is a technique used in artificial neural networks to counteract this. This technique works by disabling some neurons randomly at each iteration of training in order to prevent increased dependency on each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_02():\n",
    "    print('\\n Model 04 \\n')      \n",
    "    # Initialising Model\n",
    "    model = Sequential()\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    model.add(Dense(units = 162, input_shape=(9,), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # Adding the second hidden layer\n",
    "    model.add(Dense(units = 142, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # Adding hidden layer\n",
    "    model.add(Dense(units = 72, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # Adding hidden layer\n",
    "    model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # Adding BatchNormalization\n",
    "    model.add(BatchNormalization())\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    model.summary()\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we start to apply the dropout layer after our first hidden layer till our output layer (excluding output layer). Using 0.1 means that 1% of the neurons will be disabled at each iteration.\n",
    "\n",
    "\n",
    "now lets try with 500 epochs and batch_size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_03 = KerasClassifier(build_fn = build_model_02, \n",
    "                           epochs=500, \n",
    "                           batch_size = 10)\n",
    "history_03 = model_03.fit(X_train_scaled, \n",
    "                          y_train_encoded, \n",
    "                          validation_data=(X_test_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try with reduced batch_size (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_04 = KerasClassifier(build_fn = build_model_02, \n",
    "                           epochs=500, \n",
    "                           batch_size = 5)\n",
    "history_04 = model_04.fit(X_train_scaled, \n",
    "                          y_train_encoded, \n",
    "                          validation_data=(X_test_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot to get a clear understanding of each models accuracy and loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_lss(history_03,\n",
    "             history_04,\n",
    "             \"Batch size = 10 (Model_03)  Accuracy\",\n",
    "             \"Batch size = 10 (Model_03) Loss\",\n",
    "             \"Batch size = 5 (Model_04)   Accuracy\",\n",
    "             \"Batch size = 5 (Model_04) Loss\",\n",
    "             \"Affects of batch size on accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets create a plot to visualize and compaire the 4 models Training Accuracy, Validation accurarcy, Training Loss, Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(25,15))\n",
    "\n",
    "# Training Accuracy\n",
    "ax[0, 0].plot(history_01.history['accuracy'], label='Model 01')\n",
    "ax[0, 0].plot(history_02.history['accuracy'], label='Model 02')\n",
    "ax[0, 0].plot(history_03.history['accuracy'], label='Model 03')\n",
    "ax[0, 0].plot(history_04.history['accuracy'], label='Model 04')\n",
    "ax[0, 0].set_title('Training Accuracy\\n\\n\\n')\n",
    "ax[0, 0].set_xlabel('Epoch', fontsize=18)\n",
    "ax[0, 0].set_ylabel('Accuracy', fontsize=18)\n",
    "ax[0, 0].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# Validation accurarcy\n",
    "ax[0, 1].plot(history_01.history['val_accuracy'], label='Model 01', linestyle='--')\n",
    "ax[0, 1].plot(history_02.history['val_accuracy'], label='Model 02', linestyle='--')\n",
    "ax[0, 1].plot(history_03.history['val_accuracy'], label='Model 03', linestyle='--')\n",
    "ax[0, 1].plot(history_04.history['val_accuracy'], label='Model 04', linestyle='--')\n",
    "ax[0, 1].set_title('Validation Accurarcy\\n\\n\\n')\n",
    "ax[0, 1].set_xlabel('Epoch', fontsize=18)\n",
    "ax[0, 1].set_ylabel('Accuracy', fontsize=18)\n",
    "ax[0, 1].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# Training Loss\n",
    "ax[1, 0].plot(history_01.history['loss'], label='Model 01')\n",
    "ax[1, 0].plot(history_02.history['loss'], label='Model 02')\n",
    "ax[1, 0].plot(history_03.history['loss'], label='Model 03')\n",
    "ax[1, 0].plot(history_04.history['loss'], label='Model 04')\n",
    "ax[1, 0].set_title('Training Loss\\n\\n\\n')\n",
    "ax[1, 0].set_xlabel('Epoch', fontsize= 18)\n",
    "ax[1, 0].set_ylabel('Loss', fontsize= 18)\n",
    "ax[1, 0].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# Validation Loss\n",
    "ax[1, 1].plot(history_01.history['val_loss'], label='Model 01', linestyle='--')\n",
    "ax[1, 1].plot(history_02.history['val_loss'], label='Model 02', linestyle='--')\n",
    "ax[1, 1].plot(history_03.history['val_loss'], label='Model 03', linestyle='--')\n",
    "ax[1, 1].plot(history_04.history['val_loss'], label='Model 04', linestyle='--')\n",
    "ax[1, 1].set_title('Validation Loss\\n\\n\\n')\n",
    "ax[1, 1].set_xlabel('Epoch', fontsize=18)\n",
    "ax[1, 1].set_ylabel('Loss', fontsize=18)\n",
    "ax[1, 1].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.subplots_adjust(left=0.3,\n",
    "                    bottom=0.3, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets print the training and test accuracy of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(\"Training set accuracy - Model 01: \", history_01.history.get('accuracy')[-1])\n",
    "print(\"Test set accuracy - Model 01: \", history_01.history.get('val_accuracy')[-1])\n",
    "print('\\n')\n",
    "\n",
    "print(\"Training set accuracy - Model 02: \", history_02.history.get('accuracy')[-1])\n",
    "print(\"Test set accuracy - Model 02: \", history_02.history.get('val_accuracy')[-1])\n",
    "print('\\n')\n",
    "\n",
    "print(\"Training set accuracy - Model 03: \", history_03.history.get('accuracy')[-1])\n",
    "print(\"Test set accuracy - Model 03: \", history_03.history.get('val_accuracy')[-1])\n",
    "print('\\n')\n",
    "\n",
    "print(\"Training set accuracy - Model 04: \", history_04.history.get('accuracy')[-1])\n",
    "print(\"Test set accuracy - Model 04: \", history_04.history.get('val_accuracy')[-1])\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 4 is more accurate than the other three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Results\n",
    "\n",
    "creating a dataframe with the Training Accuracy, Test Accuracy, Training Loss and Test Loss of all the 4 models to compare and visualice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Model_01', history_01.history.get('accuracy')[-1], history_01.history.get('val_accuracy')[-1],\n",
    "           history_01.history.get('loss')[-1], history_01.history.get('val_loss')[-1]),\n",
    "          ('Model_02', history_02.history.get('accuracy')[-1], history_02.history.get('val_accuracy')[-1],\n",
    "          history_02.history.get('loss')[-1], history_02.history.get('val_loss')[-1]),\n",
    "          ('Model_03', history_03.history.get('accuracy')[-1], history_03.history.get('val_accuracy')[-1],\n",
    "          history_03.history.get('loss')[-1], history_03.history.get('val_loss')[-1]),\n",
    "          ('Model_04', history_04.history.get('accuracy')[-1], history_04.history.get('val_accuracy')[-1],\n",
    "          history_04.history.get('loss')[-1], history_04.history.get('val_loss')[-1])\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(data = models, columns=['ANN', 'Training Accuracy', 'Test Accuracy', 'Training Loss', 'Test Loss'])\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(predict)\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualizing Models Performance\n",
    "\n",
    "creating two plots to visualize the 4 models accuracy and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [17, 8]\n",
    "#plt.style.use('fivethirtyeight') \n",
    "predict.reset_index().plot(x=\"ANN\", y=[\"Training Accuracy\", \"Test Accuracy\"], kind=\"bar\")\n",
    "plt.title(\"\\n Train and Test Accuracy \\n\\n\", fontsize = 30)\n",
    "plt.xlabel(\"\\n Models\", fontsize = 20)\n",
    "plt.ylabel(\"Accuracy\\n\", fontsize = 20)\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "           ncol=4, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [17, 8]\n",
    "#plt.style.use('fivethirtyeight') \n",
    "predict.reset_index().plot(x=\"ANN\", y=[\"Training Accuracy\", \"Test Accuracy\", 'Training Loss', 'Test Loss'], kind=\"bar\")\n",
    "plt.title(\"\\n Model Accuracy and Loss \\n\\n\", fontsize = 30)\n",
    "plt.xlabel(\"\\n Models\", fontsize = 20)\n",
    "plt.ylabel(\"Accuracy and Loss\\n\", fontsize = 20)\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "           ncol=4, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning with Grid Search\n",
    "\n",
    "A grid search is used to evaluate different configurations for our neural network model.\n",
    "\n",
    " The combination that provides the best-estimated performance will be reported.\n",
    "\n",
    "The build_model() function is defined to take 4 arguments activation, init, optimizer and loss, four of them must have default values. This lets us evaluate the effect of the use of different optimization algorithms and weight initialization schemes for our network.\n",
    "\n",
    " We define the arrays of values for the parameter we wish to search after the creation of our model, specifically:\n",
    "\n",
    "Optimizers for searching different weight values.\n",
    "Initializers using different schemes for preparing the network weights\n",
    "Epochs for training the model for different number of exposures to the training dataset.\n",
    "Batches for varying the number of samples prior to a weight update.\n",
    "The options are described into a dictionary and passed to the configuration of the GridSearchCV scikit-learn class. This class will assess a version of our neural network model for each and every combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the ANN\n",
    "def build_model(activation='relu',init='glorot_uniform',optimizer='adam',loss='categorical_crossentropy'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # adding input layer and first hidden layer\n",
    "    model.add(Dense(units = 32, kernel_initializer = init, activation = activation, input_dim = 9))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 24, kernel_initializer = init, activation = activation))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 16, kernel_initializer = init, activation = activation))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 8, kernel_initializer = init, activation = activation))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # Adding BatchNormalization\n",
    "    model.add(BatchNormalization())\n",
    "    # adding output layer\n",
    "    model.add(Dense(units = 6, kernel_initializer = init, activation = activation))\n",
    "    # summary\n",
    "    model.summary()\n",
    "    # compile model\n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create KerasClassifier object\n",
    "\n",
    "A special wrapper class from Keras than enmeshes the Scikit-learn classifier PI with Keras parametric models. Various model parameters corresponding to the build_model function and other hyperparameters can be passed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = build_model)\n",
    "model.evaluate= \"classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching the following hyperparameters\n",
    "\n",
    "activation,\n",
    "loss,\n",
    "optimizer type,\n",
    "initialization method,\n",
    "batch size,\n",
    "number of epochs\n",
    "\n",
    "Creating a dictionary of search parameters and passing it onto the Scikit-learn GridSearchCV estimator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The independent variables used to train the ANN model is represented by X_train_scaled  and the column being predicted is represented by y_train_encoded. The number of times we pass our full dataset through the ANN is expressed by Epochs. The number of observations required after which the weights will be updated is described by Batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimention_function(activations,loss,optimizers,initializers,batches,epochs):\n",
    "    parameters_grid = dict(activation = activations,\n",
    "                           loss = loss,\n",
    "                           optimizer = optimizers,\n",
    "                           epochs = epochs,\n",
    "                           batch_size = batches,\n",
    "                           init = initializers)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = model,\n",
    "                               param_grid = parameters_grid)\n",
    "    # Fitting our ANN to the training set\n",
    "    grid_search = grid_search.fit(X_train_scaled, \n",
    "                                  y_train_encoded, \n",
    "                                  validation_data=(X_test_scaled, y_test_encoded))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to shorten the time taken for the process small number of epochs are used also different dimensions are passed here. This would assist in removing dimensions with poor performance and try more epochs with less batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions to search over\n",
    "activations = ['relu','tanh','sigmoid']\n",
    "loss = ['categorical_crossentropy','mean_squared_error',\"binary_crossentropy\"]\n",
    "optimizers = ['adam', 'rmsprop','sgd']\n",
    "initializers = ['glorot_uniform', 'normal', 'uniform']\n",
    "batches = [10, 32, 100]\n",
    "epochs = [5, 10, 15]\n",
    "\n",
    "# calling funstion\n",
    "grid_search_01 = dimention_function(activations,loss,optimizers,initializers,batches,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize results\n",
    "\n",
    "here we are defining a function to summarize the results and create a dataframe with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(best_parameters,best_accuracy,grid_search):\n",
    "    print(\"_\"*110,'\\n')\n",
    "    print('Best Parameters after tuning: {}'.format(best_parameters))\n",
    "    print('Best Accuracy after tuning: {}'.format(best_accuracy))\n",
    "    print('\\n',\"_\"*110,'\\n')\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    params = grid_search.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    print(\"_\"*110,'\\n')\n",
    "    data_f = pd.DataFrame(params)\n",
    "    data_f['Mean'] = means\n",
    "    data_f['Std. Dev'] = stds\n",
    "    return data_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the best selection of parameters using best_params from the grid search object. Likewise we use the best_score_ to get the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_01 = grid_search_01.best_params_\n",
    "best_accuracy_01 = grid_search_01.best_score_\n",
    "\n",
    "data_f_01 = summarize_results(best_parameters_01,\n",
    "                              best_accuracy_01,\n",
    "                              grid_search_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch_plot(data_f):\n",
    "    plt.figure(figsize=(17,10))\n",
    "    data_f.plot(x='Std. Dev' )\n",
    "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "               loc='lower left', \n",
    "               ncol=2, \n",
    "               mode=\"expand\", \n",
    "               borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_plot(data_f_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets run with the same dimensions to see whether its giving the same result or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions to search over\n",
    "activations = ['relu','tanh','sigmoid']\n",
    "loss = ['categorical_crossentropy','mean_squared_error',\"binary_crossentropy\"]\n",
    "optimizers = ['adam', 'rmsprop','sgd']\n",
    "initializers = ['glorot_uniform', 'normal', 'uniform']\n",
    "batches = [10, 32, 100]\n",
    "epochs = [5, 10, 15]\n",
    "\n",
    "# calling funstion\n",
    "grid_search_01_second = dimention_function(activations,\n",
    "                                           loss,\n",
    "                                           optimizers,\n",
    "                                           initializers,\n",
    "                                           batches,\n",
    "                                           epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the best selection of parameters using best_params from the grid search object. Likewise we use the best_score_ to get the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_01_second = grid_search_01_second.best_params_\n",
    "best_accuracy_01_second = grid_search_01_second.best_score_\n",
    "\n",
    "data_f_01_second = summarize_results(best_parameters_01_second,\n",
    "                                     best_accuracy_01_second,\n",
    "                                     grid_search_01_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_plot(data_f_01_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we added one more activations dimension and two more loss dimension to find the best one. I am trying with batch size 10 and epochs 15 because it helps to finish our process bit fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions to search over\n",
    "activations = ['relu','tanh','sigmoid','softmax']\n",
    "loss = ['categorical_crossentropy','mean_squared_error',\"binary_crossentropy\",\n",
    "        'sparse_categorical_crossentropy','kullback_leibler_divergence']\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizers = ['adam', 'rmsprop','sgd',opt]\n",
    "initializers = ['glorot_uniform', 'normal', 'uniform','zeros','random_normal']\n",
    "batches = [10]\n",
    "epochs = [15]\n",
    "\n",
    "# calling funstion\n",
    "grid_search_01_third = dimention_function(activations,\n",
    "                                          loss,\n",
    "                                          optimizers,\n",
    "                                          initializers,\n",
    "                                          batches,\n",
    "                                          epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_01_third = grid_search_01_third.best_params_\n",
    "best_accuracy_01_third = grid_search_01_third.best_score_\n",
    "\n",
    "data_f_01_third = summarize_results(best_parameters_01_third, \n",
    "                                    best_accuracy_01_third, \n",
    "                                    grid_search_01_third)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_plot(data_f_01_third)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print Best Parameters and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print('Best Parameters_01 after tuning: {}'.format(best_parameters_01))\n",
    "print('Best Accuracy_01 after tuning: {}'.format(best_accuracy_01))\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print('Best Parameters_01_second after tuning: {}'.format(best_parameters_01_second))\n",
    "print('Best Accuracy_01_second after tuning: {}'.format(best_accuracy_01_second))\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print('Best Parameters_01_third after tuning: {}'.format(best_parameters_01_third))\n",
    "print('Best Accuracy_01_third after tuning: {}'.format(best_accuracy_01_third))\n",
    "print('\\n',\"_\"*110,'\\n')\n",
    "print('Best Parameters_new_model after tuning: {}'.format(best_parameters_new_model))\n",
    "print('Best Accuracy_new_model after tuning: {}'.format(best_accuracy_new_model))\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows the best parameters in each approach which remain same except in the case of loss.\n",
    "\n",
    "Now lets create a new model and add the dimensions(parameters) that we find as best and add loss as 'mean_squared_error' because 3 out of 4 times shows mean_squared_error as best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the ANN\n",
    "def build_model_binary_crossentropy():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # adding input layer and first hidden layer\n",
    "    model.add(Dense(units = 32, kernel_initializer = 'glorot_uniform', activation = 'tanh', input_dim = 9))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 24, kernel_initializer ='glorot_uniform', activation = 'tanh'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 16, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 8, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # Adding BatchNormalization\n",
    "    model.add(BatchNormalization())\n",
    "    # adding output layer\n",
    "    model.add(Dense(units = 6, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "    model.summary()\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_binary_crossentropy = KerasClassifier(build_fn = build_model_binary_crossentropy, \n",
    "                                                epochs=500, \n",
    "                                                batch_size = 10)\n",
    "new_model_binary_crossentropy_history = new_model_binary_crossentropy.fit(X_train_scaled, \n",
    "                                                                          y_train_encoded, \n",
    "                                                                          validation_data=(X_test_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set accuracy (loss dimension 'binary_crossentropy'): \", new_model_binary_crossentropy_history.history.get('accuracy')[-1])\n",
    "print(\"Test set accuracy (loss dimension 'binary_crossentropy'): \", new_model_binary_crossentropy_history.history.get('val_accuracy')[-1])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can try with the other loss dimension ('mean_squared_error') to see which one is more fit with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the ANN\n",
    "def build_model_mean_squared_error():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # adding input layer and first hidden layer\n",
    "    model.add(Dense(units = 32, kernel_initializer = 'glorot_uniform', activation = 'tanh', input_dim = 9))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 24, kernel_initializer ='glorot_uniform', activation = 'tanh'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 16, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # adding layer\n",
    "    model.add(Dense(units = 8, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "    # adding dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    # Adding BatchNormalization\n",
    "    model.add(BatchNormalization())\n",
    "    # adding output layer\n",
    "    model.add(Dense(units = 6, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "    model.summary()\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_mean_squared_error = KerasClassifier(build_fn = build_model_mean_squared_error, \n",
    "                                               epochs=500, \n",
    "                                               batch_size = 10)\n",
    "new_model_mean_squared_error_history = new_model_mean_squared_error.fit(X_train_scaled, \n",
    "                                                                        y_train_encoded, \n",
    "                                                                        validation_data=(X_test_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print Training, Test Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set accuracy (loss dimension'mean_squared_error'): \", new_model_mean_squared_error_history.history.get('accuracy')[-1])\n",
    "print(\"Test set accuracy (loss dimension'mean_squared_error'): \", new_model_mean_squared_error_history.history.get('val_accuracy')[-1])\n",
    "print(\"Training set loss (loss dimension'mean_squared_error'): \", new_model_mean_squared_error_history.history.get('loss')[-1])\n",
    "print(\"Test set loss (loss dimension'mean_squared_error'): \", new_model_mean_squared_error_history.history.get('val_loss')[-1])\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize Training, Test Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_lss(new_model_binary_crossentropy_history,\n",
    "             new_model_mean_squared_error_history,\n",
    "             \"Accuracy (loss dimension 'binary_crossentropy')\",\n",
    "             \"Loss (loss dimension 'binary_crossentropy')\",\n",
    "             \"Accuracy (loss dimension 'mean_squared_error')\",\n",
    "             \"Loss (loss dimension 'mean_squared_error')\",\n",
    "             \"Affects by loss function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize to get a clear understanding of each Training, Test Accuracy and Loss of both model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(25,15))\n",
    "\n",
    "# Training Accuracy\n",
    "ax[0, 0].plot(new_model_binary_crossentropy_history.history['accuracy'], label='binary_crossentropy')\n",
    "ax[0, 0].plot(new_model_mean_squared_error_history.history['accuracy'], label='mean_squared_error')\n",
    "ax[0, 0].set_title('Training Accuracy\\n\\n')\n",
    "ax[0, 0].set_xlabel('Epoch', fontsize=18)\n",
    "ax[0, 0].set_ylabel('Accuracy', fontsize=18)\n",
    "ax[0, 0].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# Validation accurarcy\n",
    "ax[0, 1].plot(new_model_binary_crossentropy_history.history['val_accuracy'], label='binary_crossentropy', linestyle='--')\n",
    "ax[0, 1].plot(new_model_mean_squared_error_history.history['val_accuracy'], label='mean_squared_error', linestyle='--')\n",
    "ax[0, 1].set_title('Validation Accurarcy\\n\\n')\n",
    "ax[0, 1].set_xlabel('Epoch', fontsize=18)\n",
    "ax[0, 1].set_ylabel('Accuracy', fontsize=18)\n",
    "ax[0, 1].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# Training Loss\n",
    "ax[1, 0].plot(new_model_binary_crossentropy_history.history['loss'], label='binary_crossentropy')\n",
    "ax[1, 0].plot(new_model_mean_squared_error_history.history['loss'], label='mean_squared_error')\n",
    "ax[1, 0].set_title('Training Loss\\n\\n')\n",
    "ax[1, 0].set_xlabel('Epoch', fontsize= 18)\n",
    "ax[1, 0].set_ylabel('Loss', fontsize= 18)\n",
    "ax[1, 0].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# Validation Loss\n",
    "ax[1, 1].plot(new_model_binary_crossentropy_history.history['val_loss'], label='binary_crossentropy', linestyle='--')\n",
    "ax[1, 1].plot(new_model_mean_squared_error_history.history['val_loss'], label='mean_squared_error', linestyle='--')\n",
    "ax[1, 1].set_title('Validation Loss\\n\\n')\n",
    "ax[1, 1].set_xlabel('Epoch', fontsize=18)\n",
    "ax[1, 1].set_ylabel('Loss', fontsize=18)\n",
    "ax[1, 1].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.subplots_adjust(left=0.3,\n",
    "                    bottom=0.3, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this its clear that mean_squared_error is better than binary_crossentropy. \n",
    "In order to make sure that the trained model has no outlier predictions with too many errors MSE is very effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20\n",
    "\n",
    "### Creating our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Model\n",
    "final_model = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "final_model.add(Dense(units = 24, input_shape=(9,), kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "# adding dropout layer\n",
    "final_model.add(Dropout(0.1))\n",
    "# Adding the second hidden layer\n",
    "final_model.add(Dense(units = 20, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "# adding dropout layer\n",
    "final_model.add(Dropout(0.1))\n",
    "# Adding hidden layer\n",
    "final_model.add(Dense(units = 14, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "# adding dropout layer\n",
    "final_model.add(Dropout(0.1))\n",
    "# Adding hidden layer\n",
    "final_model.add(Dense(units = 8, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "# adding dropout layer\n",
    "final_model.add(Dropout(0.1))\n",
    "# Adding BatchNormalization\n",
    "#final_model.add(BatchNormalization())\n",
    "# Adding the output layer\n",
    "final_model.add(Dense(units = 6, kernel_initializer = 'glorot_uniform', activation = 'tanh'))\n",
    "# summary\n",
    "final_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use 1500 epochs and batch_size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "final_model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error' , metrics = ['accuracy'])\n",
    "final_model_history = final_model.fit(X_train_scaled,\n",
    "                                      y_train_encoded,\n",
    "                                      validation_data=(X_test_scaled, y_test_encoded),\n",
    "                                      batch_size = 10,\n",
    "                                      epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n',\"_\"*110,'\\n')\n",
    "print(\"Training set - final_Model: \", final_model_history.history.get('accuracy')[-1])\n",
    "print(\"Test set - final_Model: \", final_model_history.history.get('val_accuracy')[-1])\n",
    "print('\\n',\"_\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the final loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_\"*110)\n",
    "print('\\nModel\\n')\n",
    "final_loss, final_accuracy = final_model.evaluate(X_test_scaled, y_test_encoded)\n",
    "print('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_lss_oneTry(final_model_history,\"Final Model Accuracy and loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualisation showing our final model training, testing accuracy and training, testing loss. Training accuracy is continuously increasing but testing accuracy is slitely decreasing after 300 epochs. Training loss is continuously decreasing but testing loss is slitly increasing after 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glasses = ['Building windows\\n float processed', 'Building windows\\n non float processed', 'Vehicle windows\\n float processed', 'Containers', 'Tableware', 'Headlamps']\n",
    "Y_true = np.argmax(y_test_encoded, axis=1)\n",
    "# Model 01\n",
    "Y_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "cm = confusion_matrix(Y_true, Y_pred)\n",
    "plt.figure(figsize=(20, 10))\n",
    "# increasing font size\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.heatmap(cm, cmap=\"RdYlGn\" , annot=True, square=True, xticklabels=glasses, yticklabels=glasses,linewidths=.5)\n",
    "ax.set_ylabel('Actual', fontsize=20)\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "plt.title('Model Accuracy\\n', fontsize=30)\n",
    "plt.show()\n",
    "print('\\n',\"_\"*110,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see we have a lot more accurate prediction than out first model. we have high accuracy in Building windows float processed, Building windows non float processed and Headlamps and the least accurate predictions are in Vehicle windows float processed and Containers(No accurate prediction).\n",
    "\n",
    "lets print first model accuracy and last model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_\"*110)\n",
    "print('\\n****** First Model *******\\n')\n",
    "final_loss_fm1, final_accuracy_fm1 = first_model.evaluate(X_test_scaled, y_test_encoded)\n",
    "print('Final Loss: {}, Final Accuracy: {}'.format(final_loss_fm1, final_accuracy_fm1))\n",
    "print(\"_\"*110)\n",
    "print('\\n****** Last Model *******\\n')\n",
    "final_loss, final_accuracy = final_model.evaluate(X_test_scaled, y_test_encoded)\n",
    "print('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))\n",
    "print(\"_\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see we improved our model a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tried many different ways of approches to improve our model and we success in that process. we can clearly see the difference between out 1st and final model accuracy and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refrance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorialspoint, 2021. Seaborn - Visualizing Pairwise Relationship - Tutorialspoint [WWW Document]. URL https://www.tutorialspoint.com/seaborn/seaborn_visualizing_pairwise_relationship.htm (accessed 5.15.21).\n",
    "\n",
    "ankthon, 2020. Find duplicate rows in a Dataframe based on all or selected columns. GeeksforGeeks. URL https://www.geeksforgeeks.org/find-duplicate-rows-in-a-dataframe-based-on-all-or-selected-columns/ (accessed 5.18.21).\n",
    "\n",
    "Singh, R., 2020. Exploratory Data Analysis(EDA) from Scratch | With Pythin Implementation. Analytics Vidhya. URL https://www.analyticsvidhya.com/blog/2020/08/exploratory-data-analysiseda-from-scratch-in-python/ (accessed 5.18.21).\n",
    "\n",
    "Patil, P., 2018. What is Exploratory Data Analysis? [WWW Document]. Medium. URL https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15 (accessed 5.18.21).\n",
    "\n",
    "seaborn, 2020. seaborn.violinplot — seaborn 0.11.1 documentation [WWW Document]. URL https://seaborn.pydata.org/generated/seaborn.violinplot.html (accessed 5.18.21).\n",
    "\n",
    "carladasilvamatos, 2020. How to customize a heat map with seaborn [WWW Document]. Carla da Silva Matos. URL https://www.carladasilvamatos.com/blog/2019/12/25/lnn5xyodn2kv4i0o2mg9agdw07qgo0 (accessed 5.18.21).\n",
    "\n",
    "Kington, J., 2012. python - Improve subplot size/spacing with many subplots in matplotlib [WWW Document]. Stack Overflow. URL https://stackoverflow.com/questions/6541123/improve-subplot-size-spacing-with-many-subplots-in-matplotlib (accessed 5.19.21)\n",
    "\n",
    "Lynn, S. (2021) Bar Plots in Python using Pandas DataFrames | Shane Lynn. Available at: https://www.shanelynn.ie/bar-plots-in-python-using-pandas-dataframes/ (Accessed: 24 May 2021).\n",
    "\n",
    "Team, K. (2021) Keras documentation: Layer activation functions. Available at: https://keras.io/api/layers/activations/ (Accessed: 26 May 2021).\n",
    "\n",
    "REDOC-ER (2021) ‘EDA: Exploratory Data Analysis | Introduction to Exploratory Data Analysis’, Analytics Vidhya, 12 February. Available at: https://www.analyticsvidhya.com/blog/2021/02/introduction-to-exploratory-data-analysis-eda/ (Accessed: 27 May 2021).\n",
    "\n",
    "ashwinsharmap (2020) ‘StandardScaler, MinMaxScaler and RobustScaler techniques - ML’, GeeksforGeeks, 15 July. Available at: https://www.geeksforgeeks.org/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/ (Accessed: 28 May 2021).\n",
    "\n",
    "Coder’s Digest (2020) Outlier detection techniques(python)| how to avoid outliers without deleting it. Available at: https://www.youtube.com/watch?v=NPibqifVAW4 (Accessed: 28 May 2021).\n",
    "\n",
    "Dan, A. (2020) Exploratory Data Analysis (EDA) in Python, Medium. Available at: https://medium.com/@atanudan/exploratory-data-analysis-eda-in-python-893f963cc0c0 (Accessed: 30 May 2021).\n",
    "\n",
    "Brownlee, J. (2016) ‘Use Keras Deep Learning Models with Scikit-Learn in Python’, Machine Learning Mastery, 30 May. Available at: https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/ (Accessed: 30 May 2021).\n",
    "Mwiti, D. (2021) Introduction to Deep Learning with Keras, Medium. Available at: https://heartbeat.fritz.ai/introduction-to-deep-learning-with-keras-c7c3d14e1527 (Accessed: 31 May 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
